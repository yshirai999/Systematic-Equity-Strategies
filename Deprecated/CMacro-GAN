import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils import spectral_norm

# ------------------------
# Utility: Self-Attention Block with Causality Mask
# ------------------------
class SelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, x):
        seq_len = x.size(1)
        mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()
        attn_out, _ = self.attn(x, x, x, attn_mask=mask)
        return self.norm(x + attn_out)

# ------------------------
# Encoder for Conditioning Data
# ------------------------
class ConditioningEncoder(nn.Module):
    def __init__(self, num_assets, window, num_macro):
        super().__init__()
        self.cnn = nn.Sequential(
            nn.Conv1d(num_assets, 32, kernel_size=5, padding=2),
            nn.LeakyReLU(),
            nn.Conv1d(32, 64, kernel_size=3, padding=1),
            nn.LeakyReLU(),
            nn.AdaptiveAvgPool1d(1),
        )
        self.mlp_macro = nn.Sequential(
            nn.Linear(num_macro, 32),
            nn.LayerNorm(32),
            nn.LeakyReLU(),
        )
        self.output_layer = nn.Linear(64 + 32, 64)

    def forward(self, past_returns, macro):
        x = self.cnn(past_returns).squeeze(-1)  # [B, 64]
        m = self.mlp_macro(macro)               # [B, 32]
        return self.output_layer(torch.cat([x, m], dim=-1))  # [B, 64]

# ------------------------
# Generator matching Keras-style architecture
# ------------------------
class Generator(nn.Module):
    def __init__(self, latent_dim, ell, kernel_size=3):
        super().__init__()
        self.latent_dim = latent_dim
        self.ell = ell
        self.kernel_size = kernel_size

        self.linear = spectral_norm(nn.Linear(latent_dim, ell * 32))
        self.layernorm1 = nn.LayerNorm(ell * 32)
        self.relu = nn.ReLU()

        self.resblock1 = nn.ModuleList([
            nn.ModuleList([
                spectral_norm(nn.Conv1d(32, 64, kernel_size=kernel_size)),
                nn.LayerNorm(64),
                nn.ReLU(),
                spectral_norm(nn.Conv1d(64, 32, kernel_size=1)),
            ]) for _ in range(3)
        ])

        self.attn = SelfAttention(embed_dim=32, num_heads=8)

        self.resblock2 = nn.ModuleList([
            nn.ModuleList([
                spectral_norm(nn.Conv1d(32, 64, kernel_size=kernel_size)),
                nn.LayerNorm(64),
                nn.ReLU(),
                spectral_norm(nn.Conv1d(64, 32, kernel_size=1)),
            ]) for _ in range(3)
        ])

        self.final = spectral_norm(nn.Conv1d(32, 1, kernel_size=1))

    def forward(self, z):
        x = self.linear(z)
        x = self.layernorm1(x)
        x = self.relu(x)
        x = x.view(x.size(0), self.ell, 32).transpose(1, 2)  # [B, 32, ell]

        for block in self.resblock1:
            residual = x
            x = F.pad(x, (self.kernel_size - 1, 0))  # causal padding
            x = block[0](x)
            x = block[1](x.transpose(1, 2)).transpose(1, 2)
            x = block[2](x)
            x = block[3](x)
            x += residual

        x = x.transpose(1, 2)
        x = self.attn(x)
        x = x.transpose(1, 2)

        for block in self.resblock2:
            residual = x
            x = F.pad(x, (self.kernel_size - 1, 0))  # causal padding
            x = block[0](x)
            x = block[1](x.transpose(1, 2)).transpose(1, 2)
            x = block[2](x)
            x = block[3](x)
            x += residual

        x = self.final(x)
        return x.squeeze(1)

# ------------------------
# Discriminator (Conv + SpectralNorm style)
# ------------------------
class Discriminator(nn.Module):
    def __init__(self, ell):
        super().__init__()
        self.conv1 = spectral_norm(nn.Conv1d(1, 64, kernel_size=3, padding=1))
        self.norm1 = nn.LayerNorm(ell)
        self.act1 = nn.LeakyReLU(0.2)
        self.drop1 = nn.Dropout(0.3)

        self.conv2 = spectral_norm(nn.Conv1d(64, 128, kernel_size=3, padding=1))
        self.norm2 = nn.LayerNorm(ell)
        self.act2 = nn.LeakyReLU(0.2)

        self.pool = nn.AdaptiveAvgPool1d(1)
        self.out = spectral_norm(nn.Linear(128, 1))

    def forward(self, returns, cond):
        x = returns.unsqueeze(1)  # [B, 1, ell]
        x = self.conv1(x)
        x = self.norm1(x)
        x = self.act1(x)
        x = self.drop1(x)

        x = self.conv2(x)
        x = self.norm2(x)
        x = self.act2(x)

        x = self.pool(x).squeeze(-1)  # [B, 128]
        return self.out(x)

# ------------------------
# GAN Wrapper
# ------------------------
class GAN_BG_Macro(nn.Module):
    def __init__(self, latent_dim, ell, window, num_macro):
        super().__init__()
        self.encoder = ConditioningEncoder(num_assets=ell, window=window, num_macro=num_macro)
        self.generator = Generator(latent_dim, ell)
        self.discriminator = Discriminator(ell)

    def generate(self, z, past_returns, macro):
        cond = self.encoder(past_returns, macro)
        return self.generator(z)

    def discriminate(self, returns, past_returns, macro):
        cond = self.encoder(past_returns, macro)
        return self.discriminator(returns, cond)
